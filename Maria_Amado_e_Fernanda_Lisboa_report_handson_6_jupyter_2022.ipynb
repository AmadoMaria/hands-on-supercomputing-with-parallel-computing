{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Maria_Amado_e_Fernanda_Lisboa_report_handson_6_jupyter_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXATwZ8VxZG7",
        "tags": []
      },
      "source": [
        "\n",
        "\n",
        "# Hands-on 6: Portable Parallel Programming with CUDA\n",
        "\n",
        "M. Amado$^1$, F. Lisboa$^1$\n",
        "\n",
        "$^1$ Department of Computer Engenier â€“ University SENAI CIMATEC, Salvador, Bahia, Brazil  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5PcGb-Pt0_j"
      },
      "source": [
        "# Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orxl23v_xqVe"
      },
      "source": [
        "In order to optimize high cost applications, covering fine-grained and corse-grained parallelisms, NVIDIA develop the CUDA framework, that enables GPU programming. So, developers are able to use multicore and many threads benefits, being able to easily convert sequential _.c_ programs into parallelized _.cu_ ones. This practice, aims to explore CUDA, which is found simple to use and very powerful, reducing the code's execution time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGlWZ-SlQObx"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngiuqnkbvTdx"
      },
      "source": [
        "High performance programming with GPU (Graphic Process Units) uses both, multicore and many threads benefits, being ideal for processing large amounts of data. The NVIDIA's CUDA framework is an option for implementing general purpose parallel programming applications, and it also keeps code implementation simple and portable [1].\n",
        "\n",
        "Different of CPU's implementations, that work with two or four cores, GPUs architectures are equipped with hundreds of cores, that are able to run thousands of threads in parallel [2]. CUDA is a scalable parallel programming model because the same code can run with a different number of processors without the need of recompilation. In addition, CUDA applications can run also in CPUs, once it is possible to compile the same source code to run in different platforms [3].\n",
        "\n",
        "Once parallel programming with CUDA makes it possible to use many threads and process, it covers the fine-grained and corse-grained parallelisms. Which makes it ideal for high cost programs in areas such as visual computing and high arithmetic intensity.[3]\n",
        "\n",
        "Thus, this activity aims to explore CUDA implementation, parallelizing a sequential code wrote in C language."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWbtdBgpwXeJ"
      },
      "source": [
        "# Results and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing necessary libraries"
      ],
      "metadata": {
        "id": "UXO8ef2HkTxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "XHETnwsYWzD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install libomp-dev\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "FU_BLWwc8KIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install openmpi-bin\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "tWSUXEKfW3Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAXPY sequential implementation:\n"
      ],
      "metadata": {
        "id": "yWL0CwtrhtIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to exemplify the parallization with CUDA, we are going to use a sequential code implementation of SAXPY, a function in the standard Basic Linear Algebra Subroutines that stands for Single-Precision."
      ],
      "metadata": {
        "id": "BLu753JxqEmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code"
      ],
      "metadata": {
        "id": "g6Z9c0ymkjzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile saxpy.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "\n",
        "void saxpy ( int n,  float *x, float *y ) {\n",
        "\n",
        "  for (int i=0; i < n ; ++i)\n",
        "  y[i] = x[i] + y[i];\n",
        "\n",
        "}\n",
        "\n",
        "void printVector ( float *vector, int n ) {\n",
        "\n",
        "  for (int i=0; i < n ; ++i)\n",
        "  printf(\"%1.0f\\t\", vector[i]);\n",
        "\n",
        "  printf(\"\\n\\n\");\n",
        "\n",
        "}\n",
        "\n",
        "void generateVector (float *vector, int n) {\n",
        "\n",
        "for (int i=0; i < n ; ++i)\n",
        " vector[i] = i + 1;\n",
        "\n",
        "}\n",
        "\n",
        "int main (int argc, char *argv[]) {\n",
        "\n",
        "  int n = atoi(argv[1]);   \n",
        "  float *x,*y;\n",
        "\n",
        "  x = (float*) malloc(sizeof(float) * n);\n",
        "  y = (float*) malloc(sizeof(float) * n);\n",
        " \n",
        "  generateVector(x, n);\n",
        "  printVector(x, n);\n",
        "\n",
        "  generateVector(y, n);\n",
        "  printVector(y, n);\n",
        "\n",
        "  saxpy(n, x, y);\n",
        "  printVector(y, n);\n",
        " \n",
        "  free(x);\n",
        "  free(y);\n",
        "\n",
        "  return 0;\n",
        "\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw3K3PDsLwS1",
        "outputId": "744270e0-ff56-4733-9915-dd0466f4a62f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saxpy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Execution"
      ],
      "metadata": {
        "id": "3uXO_e1YkgPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc saxpy.cu -o saxpy"
      ],
      "metadata": {
        "id": "0Ry9LVByMjyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./saxpy 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MokfynpM80iU",
        "outputId": "bcee1954-9b0c-4d2c-f349-7074b00d9ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t\n",
            "\n",
            "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t\n",
            "\n",
            "2\t4\t6\t8\t10\t12\t14\t16\t18\t20\t\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAXPY CUDA implementation:"
      ],
      "metadata": {
        "id": "vkVvmfLFLEYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, the SAXPY code develop in C is converted for CUDA. One of the differences between the two implementations is the inclusion of \"cuda.h\" library and the change in file extension for \".cu\". Besides that, the kernel function needs to be marked as \"global\", and the variables _xd_ and _yd_ are used to store the data that came from the GPU. The function _cudaMemcpy_ is sends data from host to device, and from device to host, the former before executing the kernel function, and the latter, after it finishes.\n",
        "\n",
        "When it is time to call the kernel we need to specify the number of blocks, and the number of threads per block that we want to use in our GPU execution.\n"
      ],
      "metadata": {
        "id": "0lBl3ROKrF2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code"
      ],
      "metadata": {
        "id": "Lv-7ACOlkvjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile saxpy.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void saxpy(int n, float *x, float *y){\n",
        "  int i = threadIdx.x;\n",
        "\n",
        "  if(i < n)\n",
        "    y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "void printVector ( float *vector, int n ) {\n",
        "\n",
        "  for (int i=0; i < n ; ++i)\n",
        "  printf(\"%1.0f\\t\", vector[i]);\n",
        "\n",
        "  printf(\"\\n\\n\");\n",
        "\n",
        "}\n",
        "\n",
        "void generateVector (float *vector, int n) {\n",
        "\n",
        "for (int i=0; i < n ; ++i)\n",
        " vector[i] = i + 1;\n",
        "\n",
        "}\n",
        "\n",
        "int main (int argc, char *argv[]) {\n",
        "\n",
        "  int n = atoi(argv[1]);   \n",
        "  float *x,*y, *xd, *yd;\n",
        "\n",
        "  x = (float*) malloc(sizeof(float) * n);\n",
        "  y = (float*) malloc(sizeof(float) * n);\n",
        "\n",
        "  cudaMalloc( (void**)&xd, sizeof(float) * n );\n",
        "  cudaMalloc( (void**)&yd, sizeof(float) * n );\n",
        " \n",
        "  generateVector(x, n);\n",
        "  printVector(x, n);\n",
        "\n",
        "  generateVector(y, n);\n",
        "  printVector(y, n);\n",
        "\n",
        "  cudaMemcpy(xd, x, sizeof(float) * n, cudaMemcpyHostToDevice);\n",
        "  cudaMemcpy(yd, y, sizeof(float) * n, cudaMemcpyHostToDevice);\n",
        "\n",
        "  int NUMBER_OF_BLOCKS = 1;\n",
        "  int NUMBER_OF_THREADS_PER_BLOCK = n;\n",
        "\n",
        "  saxpy<<< NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK >>>(n, xd, yd);\n",
        "\n",
        "  cudaDeviceSynchronize();\n",
        "  //saxpy(n, x, y);\n",
        "  \n",
        "  cudaMemcpy(y, yd, sizeof(float) * (n), cudaMemcpyDeviceToHost);\n",
        "\n",
        "  printVector(y, n);\n",
        " \n",
        "  cudaFree(xd);\n",
        "  cudaFree(yd);\n",
        "\n",
        "  return 0;\n",
        "\n",
        "}"
      ],
      "metadata": {
        "id": "17S1VGjMBzMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3141e59d-b757-4b67-8471-95387f51bd24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saxpy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Execution"
      ],
      "metadata": {
        "id": "aIRak47GkyfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc saxpy.cu -o saxpy"
      ],
      "metadata": {
        "id": "-952QVUMQExE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./saxpy 10"
      ],
      "metadata": {
        "id": "Did4Q1VHB8xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eedddd1-df8f-4423-d7f9-7e7041cec4a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t\n",
            "\n",
            "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t\n",
            "\n",
            "2\t4\t6\t8\t10\t12\t14\t16\t18\t20\t\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAXPY CUDA implementation with unified memory:\n"
      ],
      "metadata": {
        "id": "5SWiBhXEM9VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Besides the previous implementation, we can also parallelize an application using CUDA's implementation with unified memory, which is easier to implement and brings better optimization results. To do so, it is necessary to call the function _cudaMallocManaged_ instead of _cudaMalloc_, and then we do not need to use the auxiliary variables _xd_ and _yp_ when calling the kernel function. Finally, before printing the results, we just call _cudaDeviceSynchronize_."
      ],
      "metadata": {
        "id": "ggTk-Fubv2FE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Code"
      ],
      "metadata": {
        "id": "zPTHhkWAlDkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile saxpy.cu\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <cuda.h>\n",
        "\n",
        "__global__ void saxpy(int n, float *x, float *y){\n",
        "    int i = threadIdx.x;\n",
        "    if (i < n)\n",
        "        y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "void printVector(float *vector, int n)\n",
        "{\n",
        "    for (int i = 0; i < n; ++i)\n",
        "        printf(\"%1.0f\\t\", vector[i]);\n",
        "    printf(\"\\n\\n\");\n",
        "}\n",
        "\n",
        "void generateVector(float *vector, int n)\n",
        "{\n",
        "    for (int i = 0; i < n; ++i)\n",
        "        vector[i] = i + 1;\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[])\n",
        "{\n",
        "    int n = atoi(argv[1]);\n",
        "    float *x, *y; \n",
        "\n",
        "    cudaMallocManaged(&x, sizeof(float) * n);\n",
        "    cudaMallocManaged(&y, sizeof(float) * n);\n",
        "\n",
        "    generateVector(x, n);\n",
        "    printVector(x, n);\n",
        "\n",
        "    generateVector(y, n);\n",
        "    printVector(y, n);\n",
        "\n",
        "    int NUMBER_OF_BLOCKS = 1;\n",
        "    int NUMBER_OF_THREADS_PER_BLOCK = n;\n",
        "\n",
        "    saxpy<<<NUMBER_OF_BLOCKS, NUMBER_OF_THREADS_PER_BLOCK>>>(n, x, y);\n",
        "\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    printVector(y, n);\n",
        "\n",
        "    cudaFree(x);\n",
        "    cudaFree(y);\n",
        "    \n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6tEGSMZNLxK",
        "outputId": "47080c72-0fdc-427b-b366-92490c71e348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saxpy.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Execution"
      ],
      "metadata": {
        "id": "IQmgd37GlGO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc saxpy.cu -o saxpy"
      ],
      "metadata": {
        "id": "A7nZmj68NL7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! ./saxpy 10"
      ],
      "metadata": {
        "id": "yWuusPj7FniZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9575e66-34c3-429a-a851-be89de717f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t\n",
            "\n",
            "1\t2\t3\t4\t5\t6\t7\t8\t9\t10\t\n",
            "\n",
            "2\t4\t6\t8\t10\t12\t14\t16\t18\t20\t\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EkecWU4KKvo"
      },
      "source": [
        "# Final Considerations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This practice aimed to explore the code parallelization with CUDA, a tool that enables the use of multiple threads and process. So, we found that turning a sequential _.c_ code in a _.cu_ implementation it is very simple, especially using the unified memory functions. Furthermore, CUDA presents itself as a good solution for applications that manipulate large amounts of data."
      ],
      "metadata": {
        "id": "D1i-v71H0Cy3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbxAR_nZ3ey8"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHs1Te143ezA"
      },
      "source": [
        "[1] Yang, C.-T., Huang, C.-L., & Lin, C.-F. (2011). Hybrid CUDA, OpenMP, and MPI parallel programming on multicore GPU clusters. Computer Physics Communications, 182(1), 266â€“269. doi:10.1016/j.cpc.2010.06.035Â \n",
        "\n",
        "[2] Luebke, D. (2008). CUDA: Scalable parallel programming for high-performance scientific computing. 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro. doi:10.1109/isbi.2008.4541126Â \n",
        "\n",
        "[3] Nickolls, J. (2008). Scalable parallel programming with CUDA introduction. 2008 IEEE Hot Chips 20 Symposium (HCS). doi:10.1109/hotchips.2008.7476518Â \n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "3uXO_e1YkgPt"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}