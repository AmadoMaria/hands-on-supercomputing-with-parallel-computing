{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Maria_Amado_e_Fernanda_Lisboa_report_handson_5_jupyter_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXATwZ8VxZG7",
        "tags": []
      },
      "source": [
        "# Hands-on 5: Basic Hybrid Application MPI+OpenMP\n",
        "\n",
        "M. Amado$^1$, F. Lisboa$^1$\n",
        "\n",
        "$^1$ Department of Computer Engenier – University SENAI CIMATEC, Salvador, Bahia, Brazil  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5PcGb-Pt0_j"
      },
      "source": [
        "# Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orxl23v_xqVe"
      },
      "source": [
        "A programação paralela possibilita uma redução no tempo de execução dos códigos, e consequentemente, resolução mais rápida de diversos problemas. O modelo híbrido aproveita o compartilhamento de memória entre processos de um mesmo subsistema do OpenMP e a distribuição distinta entre coleções de processos do MPI, podendo ser mais efetivo atualmente do que o uso separado dos dois paradigmas. O presente trabalho visa comparar as estratégias de otimização de um algoritmo de multiplicação de matrizes, considerando a aplicação dos paradigmas, utilizando os modelos com OpenMP, MPI e híbrido. Como já colocado em hipotése, o modelo híbrido apresenta resultados baseados nos valores mais otimizados dos paradigmas executados separadamente, apresentando uma otimização em relação ao tempo sequencial, entretanto também evidenciou o custo da troca de mensagens quando consideramos a paralelização apenas com *threads*. Dessa forma, a abordagem híbrida mostrou-se uma boa alternativa para o maior aproveitamento dos recursos computacionais disponíveis, melhorando tanto o uso da memória compartilhada quando distribuída."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGlWZ-SlQObx"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngiuqnkbvTdx"
      },
      "source": [
        "A programação paralela tem como objetivo a melhoria de desempenhos das aplicações a partir da execução de múltiplos processos [1]. possibilita uma redução no tempo de execução dos códigos, e consequentemente, resolução mais rápida de diversos problemas. Dentro desse âmbito há alguns paradigmas, sendo os mais conhecidos memória compartilhada e de memória distribuída entre os processos. Assim, programadores podem  adotar a estratégia da paralelização através da troca de mensagens, indicada para sistemas com memória distribuída, ou a estratégia de paralelização através da criação de threads, voltada para arquiteturas com memória compartilhada [2].\n",
        "\n",
        "O Paradigma de Memória Compartilhada significa que um único endereço de memória pode ser acessado por cada processador através de procedimentos sincronizados [3]. Nesse contexto, o OpenMP (Open Multi Processing) foi criado para explorar características da arquitetura de memória compartilhada, como o acesso direto à memória através do sistema com baixa latência e muita rapidez.\n",
        "\n",
        "\n",
        "O Paradigma de Memória Distribuída, se baseia, geralmente, na abordagem \"dividir para conquistar\" [3]. Dentro desse cenário surge o MPI (Messsage Passing Interface), o qual diversos processos paralelos trabalham concorrentemente em busca de um objetivo comum utilizando \"mensagens\" entre si [3]. O MPI possui uma coleção de funções, sendo suas principais de envio e recebimento de informações entre os processos. \n",
        "\n",
        "Com o avanço dos sistemas computacionais que agora contam com múltiplos processadores e compartilhamento de memória é possível utilizar essas ferramentas em conjunto para implementar programas ainda mais otimizados [2]. Nessas arquiteturas de memória compartilhada e distribuída (híbrida), a combinação da comunicação intra nós do OpenMP com a entre nós do MPI, pode prover uma exploração mais efetiva e moderna de sistemas *distributed shared-memory* (DSM) [3].\n",
        "\n",
        "A implementação de soluções híbridas, que aproveitam tanto os recursos da memória compartilhada quanto distribuída, tem como tendência a aplicação de um modelo de estrutura hierárquico, o que possibilita tanto a exploração de grãos grandes  e médios com o MPI, quanto do grão fino com o OpenMP, aproveitando, assim, as melhores características dos dois paradigmas [4].\n",
        "\n",
        "Portanto, esta prática tem como objetivo comparar as estratégias de otimização de um algoritmo de multiplicação de matrizes, considerando a aplicação dos paradigmas, utilizando os modelos com OpenMP, MPI e híbrido.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWbtdBgpwXeJ"
      },
      "source": [
        "# Results and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instalação das bibliotecas necessárias"
      ],
      "metadata": {
        "id": "UXO8ef2HkTxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "XHETnwsYWzD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install libomp-dev\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "FU_BLWwc8KIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install openmpi-bin\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "tWSUXEKfW3Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Paralelização utilizando o MPI"
      ],
      "metadata": {
        "id": "yWL0CwtrhtIN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementação da multiplicação de matrizes com MPI:"
      ],
      "metadata": {
        "id": "nCaNyixdLrMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### código"
      ],
      "metadata": {
        "id": "g6Z9c0ymkjzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mm-mpi.c\n",
        "\n",
        "/*\n",
        "File:           mm-mpi.c\n",
        "Purpose:        Parallelize matrix multiplication using MPI\n",
        "Authors:        Francisco Almeida, Domingo Giménez, José Miguel Mantas, Antonio M. Vidal\n",
        "                'Introducción a la programación paralela,\n",
        "                 Paraninfo Cengage Learning, 2008, Capítulo 6, \n",
        "                 Sección 6.3 Particionado de datos: Código 6.10\n",
        "                 Multiplicación de matrices por particionado de datos'\n",
        "Usage:\n",
        "HowToCompile:   mpicc mm-mpi.c -o mm-mpi\n",
        "HowToExecute:   mpirun -np <numberOfProcesses> ./mm-mpi <size>\n",
        "Example:        mpirun -np     4               ./mm-mpi  100\n",
        "Comments:\n",
        "                ◆ Spanish code comments;          \n",
        "*/\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <sys/time.h>\n",
        "#include <mpi.h>\n",
        "\n",
        "// multiplicación de matrices secuencial\n",
        "// por cada matriz aparece la zona de datos (a, b y c)\n",
        "// y el número de filas, de columnas y el leading dimension\n",
        "void mms(double *a, int fa, int ca, int lda, double *b, int fb, int cb, int ldb, double *c, int fc, int cc, int ldc) {\n",
        "    int i, j, k;\n",
        "    double s;\n",
        "    for (i = 0; i < fa; i++) \n",
        "        for (j = 0; j < cb; j++) {\n",
        "            s = 0.;\n",
        "            for (k = 0; k < ca; k++)\n",
        "                s += a[i * lda + k] * b[k * ldb + j];\n",
        "            c[i * ldc + j] = s;\n",
        "        }\n",
        "}\n",
        "\n",
        "// nodo es un identificador del proceso\n",
        "// y np el número total de procesos\n",
        "void mm(double *a, int fa, int ca, int lda, double *b, int fb, int cb, int ldb, double *c, int fc, int cc, int ldc, int nodo, int np) {\n",
        "    int i, j, k;\n",
        "    double s;\n",
        "    if (nodo == 0) {\n",
        "        for (i = 1; i < np; i++)\n",
        "            MPI_Send(&a[i * lda * fa / np], fa / np * ca, MPI_DOUBLE, i, 20, MPI_COMM_WORLD);\n",
        "        MPI_Bcast(b, fb * cb, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
        "    } else {\n",
        "        MPI_Recv(a, fa / np * ca, MPI_DOUBLE, 0, 20, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "        MPI_Bcast(b, fb * cb, MPI_DOUBLE, 0, MPI_COMM_WORLD);\n",
        "    }\n",
        "    mms(a, fa / np, ca, lda, b, fb, cb, ldb, c, fc / np, cc, ldc);\n",
        "    if (nodo == 0)\n",
        "        for (i = 1; i < np; i++)\n",
        "            MPI_Recv(&c[i * ldc * fc / np],fc / np * cc, MPI_DOUBLE, i, 30, MPI_COMM_WORLD, MPI_STATUS_IGNORE);\n",
        "    else\n",
        "        MPI_Send(c, fc / np * cc, MPI_DOUBLE, 0, 30, MPI_COMM_WORLD);\n",
        "}\n",
        "\n",
        "/*\n",
        "c\n",
        "c initialize - random initialization for array\n",
        "c\n",
        "*/\n",
        "\n",
        "void initialize(double *m, int f, int c, int ld) {\n",
        "  int i, j;\n",
        "\n",
        "  for (i = 0; i < f; i++) {\n",
        "    for (j = 0; j < c; j++) {  \n",
        "      m[i * ld + j] = (double)(i + j);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initializealea(double *m, int f, int c, int ld) {\n",
        "  int i, j;\n",
        "\n",
        "  for (i = 0; i < f; i++) {\n",
        "    for (j = 0; j < c; j++) {  \n",
        "      m[i * ld + j] = (double)rand() / RAND_MAX;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void escribir(double *m, int f, int c, int ld) {\n",
        "  int i, j;\n",
        "\n",
        "  for (i = 0; i < f; i++) {\n",
        "    for (j = 0; j < c; j++) {  \n",
        "      printf(\"%.4lf \",m[i * ld + j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "\n",
        "void comparar(double *m1, int fm1, int cm1, int ldm1, double *m2, int fm2, int cm2, int ldm2)\n",
        "{\n",
        "  int i, j;\n",
        "\n",
        "  for(i = 0; i < fm1; i++)\n",
        "    for(j = 0; j < cm1; j++) {\n",
        "      if(m1[i * ldm1 + j] != m2[i * ldm2 + j]) {\n",
        "        printf(\"Discrepance in %d,%d: %.8lf , %.8lf\\n\", i, j, m1[i * ldm1 + j], m2[i * ldm2 + j]);\n",
        "        return;\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc, char *argv[]) {\n",
        "  int nodo, np, i, j, fa, fal, ca, lda, fb, cb, ldb, fc, fcl, cc, ldc, N;\n",
        "  int long_name;\n",
        "  double ti, tf;\n",
        "  double *a, *b, *c, *c0;\n",
        "  char    nombre_procesador[MPI_MAX_PROCESSOR_NAME];\n",
        "  MPI_Status estado;\n",
        " \n",
        "  MPI_Init(&argc, &argv);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD, &np);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD, &nodo);\n",
        "  MPI_Get_processor_name(nombre_procesador, &long_name);\n",
        "\n",
        "// Se ejecuta con mpirun -np numeroprocesos ejecutable tamañomatriz\n",
        "\n",
        "  if (nodo == 0) {\n",
        "    N = atoi(argv[1]);\n",
        "  }\n",
        "  MPI_Bcast(&N, 1, MPI_INT, 0, MPI_COMM_WORLD);\n",
        "  \n",
        "  fa = ca = lda = fb = cb = ldb = fc = cc = ldc = N;\n",
        "  fal = N / np;\n",
        "  fcl = N / np;\n",
        "  if (nodo == 0) {\n",
        "    a = (double *) malloc(sizeof(double) * fa * ca);\n",
        "    b = (double *) malloc(sizeof(double) * fb * cb);\n",
        "    c = (double *) malloc(sizeof(double) * fc * cc);\n",
        "  } else {\n",
        "    a = (double *) malloc(sizeof(double) * fal * ca);\n",
        "    b = (double *) malloc(sizeof(double) * fb * cb);\n",
        "    c = (double *) malloc(sizeof(double) * fcl * cc);\n",
        "  }\n",
        "  \n",
        "  if (nodo == 0) {\n",
        "    c0 = (double *) malloc(sizeof(double) * fc * cc);\n",
        "    initialize(a, fa, ca, lda);\n",
        "    initialize(b, fb, cb, ldb);\n",
        "\n",
        "    mms(a, fa, ca, lda, b, fb, cb, ldb, c0, fc, cc, ldc);\n",
        "  }\n",
        "\n",
        "  MPI_Barrier(MPI_COMM_WORLD);\n",
        "\n",
        "  ti = MPI_Wtime();\n",
        "\n",
        "  mm(a, fa, ca, lda, b, fb, cb, ldb, c, fc, cc, ldc, nodo, np);\n",
        "\n",
        "  MPI_Barrier(MPI_COMM_WORLD);\n",
        "  tf = MPI_Wtime();\n",
        "  if (nodo == 0) {\n",
        "    //printf(\"(%d) Process %d, %s, Time %.6lf\\n\", N, np, nombre_procesador, tf - ti);\n",
        "    printf(\"%d\\t%f\\n\", N, tf - ti);  \n",
        "  }\n",
        "  \n",
        "  free(a);\n",
        "  free(b);\n",
        "  free(c);\n",
        "  if (nodo == 0)\n",
        "    free(c0);\n",
        "  MPI_Finalize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gw3K3PDsLwS1",
        "outputId": "93fb0ba2-caa3-4b4f-c163-c9a912252069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mm-mpi.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### execução"
      ],
      "metadata": {
        "id": "3uXO_e1YkgPt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicc mm-mpi.c -o mm-mpi\n",
        "!mpirun --allow-run-as-root -np 4 ./mm-mpi 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ry9LVByMjyD",
        "outputId": "487d55ae-19ba-46ff-d692-a1a896b31ae7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\t0.008182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementação da multiplicação de matrizes com o OpenMP:"
      ],
      "metadata": {
        "id": "vkVvmfLFLEYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### código"
      ],
      "metadata": {
        "id": "Lv-7ACOlkvjm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mm-openmp.c\n",
        "\n",
        "/*\n",
        "File:           mm-openmp.c\n",
        "Version:        Solution\n",
        "Purpose:        Matrix Multiply Sequential Algorithm in C using OpenMP\n",
        "Author:         Murilo Boratto  <muriloboratto 'at' fieb.org.br>\n",
        "Usage:\n",
        "Hotocompile:   gcc mm-openmp.c -o mm -fopenmp\n",
        "Hotoexecute:   OMP_NUM_THREADS=<threads> ./mm <size>\n",
        "               OMP_NUM_THREADS=4         ./mm  100  \n",
        "*/\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <omp.h>\n",
        "\n",
        "void initializeMatrix(int *matrix, int size)\n",
        "{\n",
        "  for (int i = 0; i < size; i++)\n",
        "    for (int j = 0; j < size; j++)\n",
        "      matrix[i * size + j] = rand() % (10 - 1) * 1;\n",
        "}\n",
        "\n",
        "void printMatrix(int *matrix, int size)\n",
        "{\n",
        "  for (int i = 0; i < size; i++)\n",
        "  {\n",
        "    for (int j = 0; j < size; j++)\n",
        "      printf(\"%d\\t\", matrix[i * size + j]);\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "  printf(\"\\n\");\n",
        "}\n",
        "\n",
        "int main (int argc, char **argv)\n",
        "{\n",
        "\n",
        " int size = atoi(argv[1]);  \n",
        " int i, j, k;\n",
        " double t1, t2;\n",
        "\n",
        " int  *A = (int *) malloc (sizeof(int)*size*size);\n",
        " int  *B = (int *) malloc (sizeof(int)*size*size);\n",
        " int  *C = (int *) malloc (sizeof(int)*size*size);\n",
        "\n",
        " initializeMatrix(A, size);\n",
        " initializeMatrix(B, size);\n",
        "\n",
        " t1 = omp_get_wtime();\n",
        "\n",
        " #pragma omp parallel for private(i, j, k)\n",
        "   for(i = 0; i < size; i++)\n",
        "    for(j = 0; j < size; j++)\n",
        "      for(k = 0; k < size; k++)\n",
        "        C[i * size + j] += A[i * size + k] * B[k * size + j];\n",
        "\n",
        " t2 = omp_get_wtime();\n",
        "\n",
        " printf(\"%d\\t%f\\n\",size, t2-t1);\n",
        "\n",
        "// printMatrix(A,size);\n",
        "// printMatrix(B,size);\n",
        "// printMatrix(C,size);\n",
        "\n",
        " return 0;\n",
        "\n",
        "}\n"
      ],
      "metadata": {
        "id": "17S1VGjMBzMu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d0d1611-00a7-4f14-ecd3-352be920e435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mm-openmp.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### execução"
      ],
      "metadata": {
        "id": "aIRak47GkyfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod 777 mm-openmp.c"
      ],
      "metadata": {
        "id": "-952QVUMQExE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gcc mm-openmp.c -o mm-openmp -fopenmp\n",
        "!OMP_NUM_THREADS=16 ./mm-openmp 100"
      ],
      "metadata": {
        "id": "Did4Q1VHB8xw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc84d3a9-a374-46e3-e5c0-bad15d281584"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\t0.005343\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementação da multiplicação de matrizes com a abordagem híbrida:"
      ],
      "metadata": {
        "id": "5SWiBhXEM9VS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### código"
      ],
      "metadata": {
        "id": "zPTHhkWAlDkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mm-mpi+openmp.c\n",
        "\n",
        "/*\n",
        "File:           mm-mpi+openmp.c\n",
        "Purpose:        Parallelize matrix multiplication using OpenMP+MPI\n",
        "Authors:        Francisco Almeida, Domingo Giménez, José Miguel Mantas, Antonio M. Vidal\n",
        "Usage:\n",
        "HowToCompile:   mpicc mm-mpi+openmp.c -o mm-mpi+openmp -fopenmp\n",
        "HowToExecute:   mpirun -np <numberOfProcesses> ./mm-mpi+openmp <size> <threads> \n",
        "Example:        mpirun -np         4           ./mm-mpi+openmp  100       16     \n",
        "Comments:\n",
        "                ◆ Spanish code comments;          \n",
        "*/\n",
        "\n",
        "#include <stdio.h>\n",
        "#include <stdlib.h>\n",
        "#include <sys/time.h>\n",
        "#include <mpi.h>\n",
        "#include <omp.h>\n",
        "\n",
        "\n",
        "void mm(double *a, int fa,int ca,int lda,double *b,int fb,int cb,int ldb,double *c,int fc,int cc,int ldc,int nodo,char *maquina)\n",
        "{\n",
        "  int i, j, k;\n",
        "  double s;\n",
        "\n",
        "#pragma omp parallel \n",
        "{\n",
        "#pragma omp for private(i,j,k,s) schedule(static)\n",
        "  for (i = 0; i < fa; i++) \n",
        "  {\n",
        "    for(j=0;j<cb;j++)\n",
        "    {\n",
        "      s=0.;\n",
        "      for (k = 0; k < ca; k++)\n",
        "\t      s = s+a[i*lda+k]*b[k*ldb+j];\n",
        "      c[i*ldc+j]=s;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "}\n",
        "\n",
        "/*\n",
        "c\n",
        "c initialize - random initialization for array\n",
        "c\n",
        "*/\n",
        "\n",
        "void initialize(double *m, int f,int c,int ld){\n",
        "  int i, j;\n",
        "\n",
        "  for (i = 0; i < f; i++)\n",
        "  {\n",
        "    for (j = 0; j < c; j++)\n",
        "    {  \n",
        "      m[i*ld+j] = (double)(i+j);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void initializealea(double *m, int f,int c,int ld){\n",
        "  int i, j;\n",
        "\n",
        "  for (i = 0; i < f; i++)\n",
        "  {\n",
        "    for (j = 0; j < c; j++)\n",
        "    {  \n",
        "      m[i*ld+j] = (double)rand()/RAND_MAX;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void escribir(double *m, int f,int c,int ld){\n",
        "  int i, j;\n",
        "\n",
        "  for (i = 0; i < f; i++)\n",
        "  {\n",
        "    for (j = 0; j < c; j++)\n",
        "    {  \n",
        "      printf(\"%.4lf \",m[i*ld+j]);\n",
        "    }\n",
        "    printf(\"\\n\");\n",
        "  }\n",
        "}\n",
        "/*\n",
        "c\n",
        "c     mseconds - returns elapsed milliseconds since Jan 1st, 1970.\n",
        "c\n",
        "*/\n",
        "long long mseconds(){\n",
        "  struct timeval t;\n",
        "  gettimeofday(&t, NULL);\n",
        "  return t.tv_sec*1000 + t.tv_usec/1000;\n",
        "}\n",
        "\n",
        "void comparar(double *m1,int fm1,int cm1,int ldm1,double *m2,int fm2,int cm2,int ldm2)\n",
        "{\n",
        "  int i,j;\n",
        "\n",
        "  for(i=0;i<fm1;i++)\n",
        "    for(j=0;j<cm1;j++)\n",
        "    {\n",
        "      if(m1[i*ldm1+j]!=m2[i*ldm2+j])\n",
        "      {\n",
        "        printf(\"Discrepance in %d,%d: %.8lf , %.8lf\\n\",i,j,m1[i*ldm1+j],m2[i*ldm2+j]);\n",
        "        return;\n",
        "      }\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(int argc,char *argv[]) {\n",
        "  int nodo,np,i, j,fa,fal,ca,lda,fb,cb,ldb,fc,fcl,cc,ldc,N,NUMTHREADS;\n",
        "  int long_name;\n",
        "  double ti,tf;\n",
        "  double *a,*b,*c,*c0;\n",
        "  char    nombre_procesador[MPI_MAX_PROCESSOR_NAME];\n",
        "  MPI_Status estado;\n",
        " \n",
        "  MPI_Init(&argc,&argv);\n",
        "  MPI_Comm_size(MPI_COMM_WORLD,&np);\n",
        "  MPI_Comm_rank(MPI_COMM_WORLD,&nodo);\n",
        "  MPI_Get_processor_name(nombre_procesador,&long_name);\n",
        "\n",
        "  if(nodo==0)\n",
        "  {\n",
        "    N=atoi(argv[1]);\n",
        "    NUMTHREADS=atoi(argv[2]);\n",
        "  }\n",
        "  MPI_Bcast(&N,1,MPI_INT,0,MPI_COMM_WORLD);\n",
        "  MPI_Bcast(&NUMTHREADS,1,MPI_INT,0,MPI_COMM_WORLD);\n",
        "  omp_set_num_threads(NUMTHREADS);\n",
        "  \n",
        "  fa=ca=lda=fb=cb=ldb=fc=cc=ldc=N;\n",
        "  fal=N/np;\n",
        "  fcl=N/np;\n",
        "  if(nodo==0)\n",
        "  {\n",
        "    a = (double *) malloc(sizeof(double)*fa*ca);\n",
        "    b = (double *) malloc(sizeof(double)*fb*cb);\n",
        "    c = (double *) malloc(sizeof(double)*fc*cc);\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    a = (double *) malloc(sizeof(double)*fal*ca);\n",
        "    b = (double *) malloc(sizeof(double)*fb*cb);\n",
        "    c = (double *) malloc(sizeof(double)*fcl*cc);\n",
        "  }\n",
        "  \n",
        "  if(nodo==0)\n",
        "  {\n",
        "    c0=(double *) malloc(sizeof(double)*fc*cc);\n",
        "    initialize(a,fa,ca,lda);\n",
        "    for(i=1;i<np;i++)\n",
        "    {\n",
        "      MPI_Send(&a[i*lda*N/np],fal*ca,MPI_DOUBLE,i,20,MPI_COMM_WORLD);\n",
        "    }\n",
        "    initialize(b,fb,cb,ldb);\n",
        "    MPI_Bcast(b,fb*cb,MPI_DOUBLE,0,MPI_COMM_WORLD);\n",
        "    mm(a,fa,ca,lda,b,fb,cb,ldb,c0,fc,cc,ldc,nodo,nombre_procesador);\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    MPI_Recv(a,fal*ca,MPI_DOUBLE,0,20,MPI_COMM_WORLD,&estado);\n",
        "    MPI_Bcast(b,fb*cb,MPI_DOUBLE,0,MPI_COMM_WORLD);\n",
        "  } \n",
        "\n",
        "  MPI_Barrier(MPI_COMM_WORLD);\n",
        "\n",
        "  ti=MPI_Wtime();\n",
        "\n",
        "  mm(a,fal,ca,lda,b,fb,cb,ldb,c,fcl,cc,ldc,nodo,nombre_procesador);\n",
        "\n",
        "  MPI_Barrier(MPI_COMM_WORLD);\n",
        "  tf=MPI_Wtime();\n",
        "  if(nodo==0)\n",
        "  {\n",
        "    //printf(\"(%d) Threads %d, Process %d, %s, Time %.6lf\\n\\n\",N, NUMTHREADS, np, nombre_procesador,tf-ti);\n",
        "    printf(\"%d\\t%f\\n\", N, tf-ti);\n",
        "    for(i=1;i<np;i++)\n",
        "    {\n",
        "      MPI_Recv(&c[i*ldc*N/np],fcl*cc,MPI_DOUBLE,i,30,MPI_COMM_WORLD,&estado);\n",
        "    }\n",
        "  }\n",
        "  else\n",
        "  {\n",
        "    MPI_Send(c,fcl*cc,MPI_DOUBLE,0,30,MPI_COMM_WORLD);\n",
        "  } \n",
        "  \n",
        "  free(a);\n",
        "  free(b);\n",
        "  free(c);\n",
        "  if(nodo==0)\n",
        "    free(c0);\n",
        "  MPI_Finalize();\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6tEGSMZNLxK",
        "outputId": "8d5305ad-ced2-4944-a85d-f56a9cc4d148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing mm-mpi+openmp.c\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### execução"
      ],
      "metadata": {
        "id": "IQmgd37GlGO9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mpicc mm-mpi+openmp.c -o mm-mpi+openmp -fopenmp\n",
        "!mpirun --allow-run-as-root -np 4 ./mm-mpi+openmp 1000 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7nZmj68NL7A",
        "outputId": "75f5e434-7514-46c8-9d5c-3c27d7b858a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\t8.321745\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Script bash para execução das implementações anteriores e geração dos gráficos comparativos\n"
      ],
      "metadata": {
        "id": "94T7hLXMW8eP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### código"
      ],
      "metadata": {
        "id": "zzYWbEVOlL2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile script.sh\n",
        "#!/bin/sh\n",
        "\n",
        "export LC_NUMERIC=\"en_US.UTF-8\"\n",
        "\n",
        "set -euo pipefail\n",
        "\n",
        "clear \n",
        "\n",
        "###################################\n",
        "# FUNCTIONS                       #\n",
        "###################################\n",
        "\n",
        "showPropeller() {\n",
        "   \n",
        "   tput civis\n",
        "   \n",
        "   while [ -d /proc/$! ]\n",
        "   do\n",
        "      for i in / - \\\\ \\|\n",
        "      do\n",
        "         printf \"\\033[1D$i\"\n",
        "         sleep .1\n",
        "      done\n",
        "   done\n",
        "   \n",
        "   tput cnorm\n",
        "}\n",
        "\n",
        "create_plot_script_time() {\n",
        "cat <<EOF >time.plt\n",
        "set title \"Execution Time\" \n",
        "set ylabel \"Time (Seconds)\"\n",
        "set xlabel \"Size\"\n",
        "\n",
        "set style line 1 lt 2 lc rgb \"cyan\"   lw 2 \n",
        "set style line 2 lt 2 lc rgb \"red\"    lw 2\n",
        "set style line 3 lt 2 lc rgb \"gold\"   lw 2\n",
        "set style line 4 lt 2 lc rgb \"green\"  lw 2\n",
        "set style line 5 lt 2 lc rgb \"blue\"   lw 2\n",
        "set style line 6 lt 2 lc rgb \"black\"  lw 2\n",
        "set terminal postscript eps enhanced color\n",
        "set output 'time.png'\n",
        "\n",
        "set xtics nomirror\n",
        "set ytics nomirror\n",
        "set key top left\n",
        "set key box\n",
        "set style data lines\n",
        "\n",
        "plot \"file_comparison.data\" using 1:2 title \"Sequential\"              ls 1 with linespoints,\\\n",
        "     \"file_comparison.data\" using 1:3 title \"T=2\"                     ls 2 with linespoints,\\\n",
        "     \"file_comparison.data\" using 1:4 title \"T=3\"                     ls 3 with linespoints,\\\n",
        "     \"file_comparison.data\" using 1:5 title \"T=4\"                     ls 4 with linespoints\n",
        "EOF\n",
        "}\n",
        "\n",
        "create_plot_script_speedup() {\n",
        "cat <<EOF >speedup.plt\n",
        "set title \"Speedup\" \n",
        "set ylabel \"Speedup\"\n",
        "set xlabel \"Size\"\n",
        "\n",
        "set style line 1 lt 2 lc rgb \"cyan\"   lw 2 \n",
        "set style line 2 lt 2 lc rgb \"red\"    lw 2\n",
        "set style line 3 lt 2 lc rgb \"gold\"   lw 2\n",
        "set style line 4 lt 2 lc rgb \"green\"  lw 2\n",
        "set style line 5 lt 2 lc rgb \"blue\"   lw 2\n",
        "set style line 6 lt 2 lc rgb \"black\"  lw 2\n",
        "set terminal postscript eps enhanced color\n",
        "set output 'speedup.png'\n",
        "\n",
        "set xtics nomirror\n",
        "set ytics nomirror\n",
        "set key top left\n",
        "set key box\n",
        "set style data lines\n",
        "\n",
        "plot \"file_speedup.data\" using 1:2 title \"T=2\"    ls 1 with linespoints,\\\n",
        "     \"file_speedup.data\" using 1:3 title \"T=3\"    ls 2 with linespoints,\\\n",
        "     \"file_speedup.data\" using 1:4 title \"T=4\"    ls 3 with linespoints\n",
        "EOF\n",
        "}\n",
        "\n",
        "################################################\n",
        "# 0. COMPILATION + PERMISSIONS  TO EXECUTE     #\n",
        "################################################\n",
        "\n",
        "compile_and_execute_openmp() {\n",
        "    # module load gcc/11.1.0\n",
        "gcc mm-openmp.c -o mm -fopenmp -O3\n",
        "chmod +x mm\n",
        "\n",
        "###################################\n",
        "# Experimental Times              #\n",
        "###################################\n",
        "\n",
        "sleep 5 > /dev/null 2>&1 &\n",
        "\n",
        "printf \"Loading...\\040\\040\" ; showPropeller\n",
        "echo \" \"\n",
        "\n",
        "for i in 100 200 300 400 500 600 700 800 900 1000\n",
        "do\n",
        "printf \"\\033[1D$i :\" \n",
        "\n",
        "for (( j=1; j<=4; j+=1 ))\n",
        "do\n",
        "OMP_NUM_THREADS=$j   ./mm               \"$i\"    >> \"file$j\"\n",
        "done\n",
        "done\n",
        "\n",
        "clear \n",
        "}\n",
        "\n",
        "compile_and_execute_mpi() {\n",
        "mpicc mm-mpi.c -o mm\n",
        "chmod +x mm\n",
        "\n",
        "\n",
        "###################################\n",
        "# Experimental Times              #\n",
        "###################################\n",
        "\n",
        "sleep 5 > /dev/null 2>&1 &\n",
        "\n",
        "printf \"Loading...\\040\\040\" ; showPropeller\n",
        "echo \" \"\n",
        "\n",
        "for i in 100 200 300 400 500 600 700 800 900 1000\n",
        "do\n",
        "printf \"\\033[1D$i :\" \n",
        "for j in {1..4}\n",
        "do\n",
        "mpirun -np \"$j\" ./mm    \"$i\"    >> \"file$j\"\n",
        "done\n",
        "done\n",
        "\n",
        "clear \n",
        "}\n",
        "\n",
        "compile_and_execute_openmpi() {\n",
        "mpicc mm-mpi+openmp.c -o mm -fopenmp\n",
        "chmod +x mm\n",
        "\n",
        "\n",
        "###################################\n",
        "# Experimental Times              #\n",
        "###################################\n",
        "\n",
        "sleep 5 > /dev/null 2>&1 &\n",
        "\n",
        "printf \"Loading...\\040\\040\" ; showPropeller\n",
        "echo \" \"\n",
        "\n",
        "for i in 100 200 300 400 500 600 700 800 900 1000\n",
        "do\n",
        "printf \"\\033[1D$i :\" \n",
        "for j in {1..4}\n",
        "do\n",
        "mpirun -np \"$j\" ./mm    \"$i\"    $j*$j >> \"file$j\"\n",
        "done\n",
        "\n",
        "done\n",
        "\n",
        "clear \n",
        "}\n",
        "\n",
        "time_comparison() {\n",
        "echo \" \"\n",
        "echo \" \"\n",
        "echo \"    ********************************\"\n",
        "echo \"    * Experimental Time Comparison *\"\n",
        "echo \"    ********************************\"\n",
        "echo \" \"\n",
        "\n",
        "pr -m -t -s\\  file1 file2 file3 file4 | awk '{print $1,\"\\t\",$2,\"\\t\",$4,\"\\t\",$6,\"\\t\",$8}' >> file_comparison.data\n",
        "\n",
        "echo \" \"\n",
        "echo \"    [#][size]       [S]\t           [T02]\t   [T03]\t  [T04]\"\n",
        "cat -n  file_comparison.data\n",
        "\n",
        "sleep 3\n",
        "\n",
        "#####################\n",
        "# SPEEDUP           #\n",
        "#####################\n",
        "\n",
        "awk '{print $1, \" \",(($2*1000)/($3*1000))}' file_comparison.data >> fspeed0 #OMP T=02 \n",
        "awk '{print $1, \" \",(($2*1000)/($4*1000))}' file_comparison.data >> fspeed1 #OMP T=03\n",
        "awk '{print $1, \" \",(($2*1000)/($5*1000))}' file_comparison.data >> fspeed2 #OMP T=04\n",
        "\n",
        "pr -m -t -s\\  fspeed0 fspeed1 fspeed2| awk '{print $1,\"\\t\",$2,\"\\t\",$4,\"\\t\",$6,\"\\t\",$8}' >> file_speedup.data\n",
        "\n",
        "# pr -m -t -s\\  fspeed0 fspeed1 fspeed2 fspeed3 fspeed4 fspeed5 fspeed6 fspeed7 fspeed8 fspeed9 fspeed10| awk '{print $1,\"\\t\",$2,\"\\t\",$4,\"\\t\",$6,\"\\t\",$8,\"\\t\",$10,\"\\t\",$12,\"\\t\",$14}' >> file_speedup.data\n",
        "\n",
        "echo \" \"\n",
        "echo \" \"\n",
        "echo \"    ********************************\"\n",
        "echo \"    * Speedup  Rate                *\"\n",
        "echo \"    ********************************\"\n",
        "echo \" \"\n",
        "echo \"    [#][size]    [ST02]\t          [ST03]\t  [ST04]\"\n",
        "cat -n file_speedup.data\n",
        "}\n",
        "\n",
        "\n",
        "ploting_setup() {\n",
        "#####################\n",
        "# PLOTING           #\n",
        "#####################\n",
        "echo \" \"\n",
        "echo \"Do you want to plot a graphic (y/n)?\"\n",
        "read resp\n",
        "\n",
        "if [[ $resp = \"y\" ]];then\n",
        "         echo \"ploting eps graphic with gnuplot...\"\n",
        "         create_plot_script_time\n",
        "         create_plot_script_speedup\n",
        "         gnuplot \"time.plt\"\n",
        "         gnuplot \"speedup.plt\"\n",
        "#rename the files\n",
        "  filename=$1\n",
        "  mv time.png  time_$filename.png\n",
        "  mv speedup.png  speedup_$filename.png\n",
        "\n",
        "fi\n",
        "}\n",
        "\n",
        "remove_unnecessary_files() {\n",
        "echo \" \"\n",
        "echo \"[Remove unnecessary files] \"\n",
        "rm -f *.txt file* fspeed* *.data mm *.plt\n",
        "echo \" \"\n",
        "\n",
        "sleep 7 > /dev/null 2>&1 &\n",
        "\n",
        "printf \"Loading...\\040\\040\" ; showPropeller\n",
        "echo \" \"\n",
        "echo \"[END] \" \n",
        "echo \" \"\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "#  execution\n",
        "\n",
        "echo \"executing OpenMP\"\n",
        "compile_and_execute_openmp\n",
        "time_comparison \n",
        "ploting_setup \"openmp\"\n",
        "remove_unnecessary_files\n",
        "\n",
        "\n",
        "echo \"executing MPI\"\n",
        "compile_and_execute_mpi\n",
        "time_comparison\n",
        "ploting_setup \"mpi\"\n",
        "remove_unnecessary_files\n",
        "\n",
        "\n",
        "echo \"executing OpenMP+MPI\"\n",
        "compile_and_execute_openmpi\n",
        "time_comparison \n",
        "ploting_setup \"openmp_mpi\"\n",
        "remove_unnecessary_files"
      ],
      "metadata": {
        "id": "3CD3EeZUBpSR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9fcef0-b616-44f5-c421-bdebecf5679e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing script.sh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### execução"
      ],
      "metadata": {
        "id": "_b5UG0mWlPKi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!bash script.sh"
      ],
      "metadata": {
        "id": "dcZahl4WCGYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fb92948-0ab7-43d5-ec3a-3b362c4eeccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[H\u001b[2J \n",
            "[Remove unnecessary files] \n",
            " \n",
            "Loading...  "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56kMBoGq8FSA"
      },
      "source": [
        "### Visualização dos resultados encontrados\n",
        "\n",
        "Tamanho do problema e tempo de execução\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Após a execução da multiplicação de matrizes utilizando OpenMP, MPI e uma abordagem híbrida, geramos os gráficos correspondentes. Consideramos uma variação no tamanho do problema de 100 a 1000, além de um número de *threads* variando de 2 a 16 e execuções por troca de mensagens com 4 processos."
      ],
      "metadata": {
        "id": "jEbkwc5pTgbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na Figura 1, observa-se a relação entre o tempo de execução e o tamanho do problema, considerando direntes quantidades de threads. Percebe-se que o tempo da execução sequencial é maior que nas execuções paralelizadas, em que 4 threads mostrou-se um número satisfatório para um tamanho de problema inferior a 800. Contudo, a mudança drástica no tempo de execução com 4 threads, aproximando seu desempenho da execução sequencial pode indicar uma interferência dos processos já em execução máquina utilizada para os testes.\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Hands-On-5-Basic-Hybrid-Application-MPI+OpenMP/material/time_openmp.png?raw=true\" />\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "**Figura 1.** *Comparação do tempo de execução da multiplicação de matrizes de diferentes tamanhos, utilizando OpenMP*"
      ],
      "metadata": {
        "id": "j1zZBBZGUl7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Já na implementação utilizando o MPI, a partir do gráfico visto na Figura 2, percebe-se que o tempo de execução sequencial também mostrou-se maior que o da implementação paralelizada, em que a utilização de 3 ou 4 processos apresenta-se mais adequada a depender do tamanho do problema.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Hands-On-5-Basic-Hybrid-Application-MPI+OpenMP/material/time_mpi.png?raw=true\" />\n",
        "</p>\n",
        "\n",
        "**Figura 2.**  *Comparação do tempo de execução da multiplicação de matrizes de diferentes tamanhos, utilizando MPI*"
      ],
      "metadata": {
        "id": "JYo6mx5Rcs5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "É  importante ressaltar que ao comparar os melhores tempos de execução da multiplicação de matrizes da abordagem com OpenMP (Figura 1) e com MPI (Figura 2), é perceptível que a execução com *threads* tem um tempo bem menor que a execução com troca de mensagens."
      ],
      "metadata": {
        "id": "qFJeTyxK9S_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Já na execução com o modelo híbrido a quantidade de 3 processos para 9 *threads* se destaca em relação a ter o menor tempo de execução para os maiores valores no tamanho do problema.\n",
        " \n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Hands-On-5-Basic-Hybrid-Application-MPI+OpenMP/material/time_openmpi.png?raw=true\" />\n",
        "</p>\n",
        "\n",
        "**Figura 3.** *Comparação do tempo de execução da multiplicação de matrizes de diferentes tamanhos, utilizando OpenMP e MPI*"
      ],
      "metadata": {
        "id": "0Apa8lFSfgNB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hz1AKGlx164H"
      },
      "source": [
        "#### Speedup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ao realizar a multiplicação de matrizes com o OpenMP, foi possível obter o grafico da Figura 4, que mostra a relação entre o tamanho do problema e o speedup para diferentes números de threads. Assim, é possível observar que os melhores resultados foram os encontrados para 3 e 4 threads, a depender do tamanho do problema.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Hands-On-5-Basic-Hybrid-Application-MPI+OpenMP/material/speedup_openmp.png?raw=true\" />\n",
        "</p>\n",
        "\n",
        "**Figura 4.** *Comparação do speedup entre a quantidade de threads utilizadas, com OpenMP.*"
      ],
      "metadata": {
        "id": "RSLAnADfRqnk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Já ao observar o *speedup* com a utilizado do MPI, percebe-se que continua a variação entre os valores de 3 e 4, nesse caso de processos.\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Hands-On-5-Basic-Hybrid-Application-MPI+OpenMP/material/speedup_mpi.png?raw=true\"/>\n",
        "</p>\n",
        "\n",
        "**Figura 5.** *Comparação do speedup entre a quantidade de threads utilizadas, com MPI.*"
      ],
      "metadata": {
        "id": "xgbH1-1XZWIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizando o gráfico de *speedup* na Figura 6 para o modelo híbrido, observa-se mais estabilidade em relação aos *speedups* apresentados nas Figuras 4 e 5 com os outros paradigmas. Entretanto, a quantidade de 3 processos com 9 *threads* é a que se mantém com maior *speedup* dentro da variação do tamanho do problema.\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Hands-On-5-Basic-Hybrid-Application-MPI+OpenMP/material/speedup_openmpi.png?raw=true\" />\n",
        "</p>\n",
        "\n",
        "**Figura 6.** *Comparação do tempo de execução da multiplicação de matrizes de diferentes tamanhos, utilizando OpenMP e MPI*"
      ],
      "metadata": {
        "id": "QuPdwdi5hzYI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EkecWU4KKvo"
      },
      "source": [
        "# Considerações Finais"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOXpKoRrJ5sm"
      },
      "source": [
        "A presente prática teve como objetivo comparar as estratégias de otimização de um algoritmo de multiplicação de matrizes, considerando a aplicação dos paradigmas, utilizando os modelos com OpenMP, MPI e híbrido. A paralelização de um código, seja através da troca de mensagens ou da criação de threads tem um grande impacto na otimização dos algoritmos. Contudo, a implementação da multiplicação de matrizes com o OpenMP apresentou um *speedup* maior em relação a execução com MPI. Além disso, a versão do código que utilizou ambos os recursos apresentou um tempo de execução intermediário quando comparado as demais, o que mostra uma otimização em relação ao tempo sequencial, mas também evidencia o custo da troca de mensagens quando consideramos a paralelização apenas com *threads*.\n",
        "Dessa forma, a abordagem híbrida mostrou-se uma boa alternativa para o maior aproveitamento dos recursos computacionais disponíveis, melhorando tanto o uso da memória compartilhada quando distribuída. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbxAR_nZ3ey8"
      },
      "source": [
        "# Referências"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHs1Te143ezA"
      },
      "source": [
        "[1] Robinson, S. (2005). Toward an optimal algorithm for matrix multiplication. SIAM news, 38(9), 1-3.\n",
        "\n",
        "[2] DE ARAUJO, L. R.; WEBER, C. M.; PUNTEL, F. E.; CHARÃO, A. S.; LIMA, J. V. F. Análise Comparativa de MPI e OpenMP em Implementações de Transposição de Matrizes para Arquitetura com Memória Compartilhada. Revista Eletrônica de Iniciação Científica em Computação, [S. l.], v. 16, n. 5, 2018. DOI: 10.5753/reic.2018.1072. Disponível em: https://sol.sbc.org.br/journals/index.php/reic/article/view/1072. Acesso em: 7 out. 2022.\n",
        "\n",
        "[3] Karniadakis, G., & Kirby II, R. (2003). Parallel Scientific Computing in C and MPI: A Seamless Approach to Parallel Algorithms and their Implementation. Cambridge: Cambridge University Press. doi:10.1017/CBO9780511812583\n",
        "\n",
        "\n",
        "[4] MENDES, Leonardo; FERREIRA, Eduardo. Programação Paralela Híbrida aplicada em um Cluster Computacional com testes em Multiplicação de Matrizes. In: ESCOLA REGIONAL DE ALTO DESEMPENHO DA REGIÃO SUL (ERAD-RS), 20. , 2020, Santa Maria. Anais [...]. Porto Alegre: Sociedade Brasileira de Computação, 2020 . p. 89-92. ISSN 2595-4164. DOI: https://doi.org/10.5753/eradrs.2020.10763."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "dGlWZ-SlQObx",
        "UXO8ef2HkTxZ",
        "yWL0CwtrhtIN",
        "g6Z9c0ymkjzK",
        "3uXO_e1YkgPt",
        "vkVvmfLFLEYl",
        "Lv-7ACOlkvjm",
        "aIRak47GkyfS",
        "5SWiBhXEM9VS",
        "zPTHhkWAlDkE",
        "IQmgd37GlGO9",
        "94T7hLXMW8eP",
        "zzYWbEVOlL2l",
        "3EkecWU4KKvo"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}