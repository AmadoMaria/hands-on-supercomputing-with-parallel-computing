{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmadoMaria/hands-on-supercomputing-with-parallel-computing/blob/master/Maria_Amado_e_Fernanda_Lisboa_report_handson_7_jupyter_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXATwZ8VxZG7",
        "tags": []
      },
      "source": [
        "\n",
        "\n",
        "# Hands-on 7: Accelerating Applications with CUDA C/C++\n",
        "\n",
        "M. Amado$^1$, F. Lisboa$^1$\n",
        "\n",
        "$^1$ Department of Computer Engenier – University SENAI CIMATEC, Salvador, Bahia, Brazil  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5PcGb-Pt0_j"
      },
      "source": [
        "# Abstract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orxl23v_xqVe"
      },
      "source": [
        "Compute Unified Device Architectures (CUDA), it is an NVIDIA's framework that enables code optimization, once developers are able to use GPU hardware resources to run a single code using multiple processors. So, this practice aims to explore the code acceleration with CUDA, once it has the power to increase code optimization through its parallelization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGlWZ-SlQObx"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngiuqnkbvTdx"
      },
      "source": [
        "The evolution of hardware resources favoured the development of General Purpose Computation on GPUs (GPGPU), which represents an enormous evolution of high-performance computing systems [1]. Therefore, the NVIDIA's CUDA framework allows the optimization of a sequential code written in C/C++, so that what was previously developed to run in the CPU, becomes able to run in parallel in the GPU.\n",
        "\n",
        "CUDA stands for Compute Unified Device Architectures, and it is a parallel programming model and environment to perform GPGPU. The GPU is a collection of many processors that follow the SIMD architecture, executing the same code, and they communicate using a shared device memory. On software level, CUDA contains a lot of parallel programming supporting primitives [2]. Thus, a CUDA program is formed by *host* code that runs in the CPU and *device* code that runs in the GPU. The functions that are called in the CPU but run in the GPU have the name of *kernels*.\n",
        "\n",
        "In this activity, the main goal it is accelerating sequential heat equation codes using the CUDA framework, separating the functions in *host* and *device* code, and using the error handling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWbtdBgpwXeJ"
      },
      "source": [
        "# Results and Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing necessary libraries"
      ],
      "metadata": {
        "id": "UXO8ef2HkTxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "XHETnwsYWzD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt install libomp-dev\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "FU_BLWwc8KIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get install openmpi-bin\n",
        "clear_output(wait=False)"
      ],
      "metadata": {
        "id": "tWSUXEKfW3Jh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Heat"
      ],
      "metadata": {
        "id": "8Qi2IjoU9toN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sequential code"
      ],
      "metadata": {
        "id": "C8WVMg4F9xVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile heat.cu\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "\n",
        "// Simple define to index into a 1D array from 2D space\n",
        "#define I2D(num, c, r) ((r)*(num)+(c))\n",
        "\n",
        "/*\n",
        " * `step_kernel_mod` is currently a direct copy of the CPU reference solution\n",
        " * `step_kernel_ref` below. Accelerate it to run as a CUDA kernel.\n",
        " */\n",
        "\n",
        "void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "  int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "\n",
        "  // loop over all points in domain (except boundary)\n",
        "  for ( int j=1; j < nj-1; j++ ) {\n",
        "    for ( int i=1; i < ni-1; i++ ) {\n",
        "      // find indices into linear memory\n",
        "      // for central point and neighbours\n",
        "      i00 = I2D(ni, i, j);\n",
        "      im10 = I2D(ni, i-1, j);\n",
        "      ip10 = I2D(ni, i+1, j);\n",
        "      i0m1 = I2D(ni, i, j-1);\n",
        "      i0p1 = I2D(ni, i, j+1);\n",
        "\n",
        "      // evaluate derivatives\n",
        "      d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];\n",
        "      d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];\n",
        "\n",
        "      // update temperatures\n",
        "      temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void step_kernel_ref(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "  int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "\n",
        "  // loop over all points in domain (except boundary)\n",
        "  for ( int j=1; j < nj-1; j++ ) {\n",
        "    for ( int i=1; i < ni-1; i++ ) {\n",
        "      // find indices into linear memory\n",
        "      // for central point and neighbours\n",
        "      i00 = I2D(ni, i, j);\n",
        "      im10 = I2D(ni, i-1, j);\n",
        "      ip10 = I2D(ni, i+1, j);\n",
        "      i0m1 = I2D(ni, i, j-1);\n",
        "      i0p1 = I2D(ni, i, j+1);\n",
        "\n",
        "      // evaluate derivatives\n",
        "      d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];\n",
        "      d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];\n",
        "\n",
        "      // update temperatures\n",
        "      temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int istep;\n",
        "  int nstep = 200; // number of time steps\n",
        "\n",
        "  // Specify our 2D dimensions\n",
        "  const int ni = 200;\n",
        "  const int nj = 100;\n",
        "  float tfac = 8.418e-5; // thermal diffusivity of silver\n",
        "\n",
        "  float *temp1_ref, *temp2_ref, *temp1, *temp2, *temp_tmp;\n",
        "\n",
        "  const int size = ni * nj * sizeof(float);\n",
        "\n",
        "  temp1_ref = (float*)malloc(size);\n",
        "  temp2_ref = (float*)malloc(size);\n",
        "  temp1 = (float*)malloc(size);\n",
        "  temp2 = (float*)malloc(size);\n",
        "\n",
        "  // Initialize with random data\n",
        "  for( int i = 0; i < ni*nj; ++i) {\n",
        "    temp1_ref[i] = temp2_ref[i] = temp1[i] = temp2[i] = (float)rand()/(float)(RAND_MAX/100.0f);\n",
        "  }\n",
        "\n",
        "  // Execute the CPU-only reference version\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_ref(ni, nj, tfac, temp1_ref, temp2_ref);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1_ref;\n",
        "    temp1_ref = temp2_ref;\n",
        "    temp2_ref= temp_tmp;\n",
        "  }\n",
        "\n",
        "  // Execute the modified version using same data\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_mod(ni, nj, tfac, temp1, temp2);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1;\n",
        "    temp1 = temp2;\n",
        "    temp2= temp_tmp;\n",
        "  }\n",
        "\n",
        "  float maxError = 0;\n",
        "  // Output should always be stored in the temp1 and temp1_ref at this point\n",
        "  for( int i = 0; i < ni*nj; ++i ) {\n",
        "    if (abs(temp1[i]-temp1_ref[i]) > maxError) { maxError = abs(temp1[i]-temp1_ref[i]); }\n",
        "  }\n",
        "\n",
        "  // Check and see if our maxError is greater than an error bound\n",
        "  if (maxError > 0.0005f)\n",
        "    printf(\"Problem! The Max Error of %.5f is NOT within acceptable bounds.\\n\", maxError);\n",
        "  else\n",
        "    printf(\"The Max Error of %.5f is within acceptable bounds.\\n\", maxError);\n",
        "\n",
        "  free( temp1_ref );\n",
        "  free( temp2_ref );\n",
        "  free( temp1 );\n",
        "  free( temp2 );\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVS0zPqJ9v7s",
        "outputId": "70381da1-9e2a-4883-9c6f-c6a44a300755"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting heat.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o heat heat.cu -run "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Iz9orAg-Haf",
        "outputId": "f4b2467e-30c0-4492-8b16-6da93b0f6fb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Max Error of 0.00000 is within acceptable bounds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With cuda"
      ],
      "metadata": {
        "id": "v5NrULaY-IMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile heat_gpu.cu\n",
        "#include <stdio.h>\n",
        "#include <math.h>\n",
        "#include <cuda.h>\n",
        "#include <assert.h>\n",
        "\n",
        "// Simple define to index into a 1D array from 2D space\n",
        "#define I2D(num, c, r) ((r)*(num)+(c))\n",
        "\n",
        "/*\n",
        " * `step_kernel_mod` is currently a direct copy of the CPU reference solution\n",
        " * `step_kernel_ref` below. Accelerate it to run as a CUDA kernel.\n",
        " */\n",
        "\n",
        "  inline cudaError_t checkCuda(cudaError_t result)\n",
        "{\n",
        "  if (result != cudaSuccess) {\n",
        "    fprintf(stderr, \"CUDA Runtime Error: %s\\n\", cudaGetErrorString(result));\n",
        "    assert(result == cudaSuccess);\n",
        "  }\n",
        "  return result;\n",
        "}\n",
        "\n",
        "__global__ void step_kernel_mod(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "  int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "  int row = blockIdx.y*blockDim.y+threadIdx.y;\n",
        "  int col = blockIdx.x*blockDim.x+threadIdx.x;\n",
        "\n",
        "  int j = row;\n",
        "  int i = col;\n",
        "\n",
        "  // loop over all points in domain (except boundary)\n",
        "  if ( j > 0 && j < nj-1) {\n",
        "    if (i > 0 && i < ni-1) {\n",
        "      // find indices into linear memory\n",
        "      // for central point and neighbours\n",
        "      i00 = I2D(ni, i, j);\n",
        "      im10 = I2D(ni, i-1, j);\n",
        "      ip10 = I2D(ni, i+1, j);\n",
        "      i0m1 = I2D(ni, i, j-1);\n",
        "      i0p1 = I2D(ni, i, j+1);\n",
        "\n",
        "      // evaluate derivatives\n",
        "      d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];\n",
        "      d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];\n",
        "\n",
        "      // update temperatures\n",
        "      temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "void step_kernel_ref(int ni, int nj, float fact, float* temp_in, float* temp_out)\n",
        "{\n",
        "  int i00, im10, ip10, i0m1, i0p1;\n",
        "  float d2tdx2, d2tdy2;\n",
        "\n",
        "\n",
        "  // loop over all points in domain (except boundary)\n",
        "  for ( int j=1; j < nj-1; j++ ) {\n",
        "    for ( int i=1; i < ni-1; i++ ) {\n",
        "      // find indices into linear memory\n",
        "      // for central point and neighbours\n",
        "      i00 = I2D(ni, i, j);\n",
        "      im10 = I2D(ni, i-1, j);\n",
        "      ip10 = I2D(ni, i+1, j);\n",
        "      i0m1 = I2D(ni, i, j-1);\n",
        "      i0p1 = I2D(ni, i, j+1);\n",
        "\n",
        "      // evaluate derivatives\n",
        "      d2tdx2 = temp_in[im10]-2*temp_in[i00]+temp_in[ip10];\n",
        "      d2tdy2 = temp_in[i0m1]-2*temp_in[i00]+temp_in[i0p1];\n",
        "\n",
        "      // update temperatures\n",
        "      temp_out[i00] = temp_in[i00]+fact*(d2tdx2 + d2tdy2);\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "int main()\n",
        "{\n",
        "  int istep;\n",
        "  int nstep = 200; // number of time steps\n",
        "\n",
        "  // Specify our 2D dimensions\n",
        "  const int ni = 200;\n",
        "  const int nj = 100;\n",
        "  float tfac = 8.418e-5; // thermal diffusivity of silver\n",
        "\n",
        "  float *temp1_ref, *temp2_ref, *temp1, *temp2, *temp_tmp;\n",
        "\n",
        "  const int size = ni * nj * sizeof(float);\n",
        "\n",
        "  temp1_ref = (float*)malloc(size);\n",
        "  temp2_ref = (float*)malloc(size);\n",
        "\n",
        "  checkCuda(cudaMallocManaged (&temp1, size));\n",
        "  checkCuda (cudaMallocManaged (&temp2, size));\n",
        "\n",
        "  cudaError_t syncErr, asyncErr;\n",
        "\n",
        "  // Initialize with random data\n",
        "  for( int i = 0; i < ni*nj; ++i) {\n",
        "    temp1_ref[i] = temp2_ref[i] = temp1[i] = temp2[i] = (float)rand()/(float)(RAND_MAX/100.0f);\n",
        "  }\n",
        "\n",
        "  // Execute the CPU-only reference version\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_ref(ni, nj, tfac, temp1_ref, temp2_ref);\n",
        "\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1_ref;\n",
        "    temp1_ref = temp2_ref;\n",
        "    temp2_ref= temp_tmp;\n",
        "  }\n",
        "\n",
        "  dim3 threads_per_block (32, 32, 1); // 1024\n",
        "  dim3 number_of_blocks (((ni*nj) / threads_per_block.x) + 1, ((ni*nj) / threads_per_block.y) + 1, 1);\n",
        "\n",
        "  // Execute the modified version using same data\n",
        "  for (istep=0; istep < nstep; istep++) {\n",
        "    step_kernel_mod<<<number_of_blocks, threads_per_block>>>(ni, nj, tfac, temp1, temp2);\n",
        "\n",
        "\n",
        "    syncErr = cudaGetLastError();\n",
        "    asyncErr = cudaDeviceSynchronize();\n",
        "    \n",
        "    if (syncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(syncErr));\n",
        "    if (asyncErr != cudaSuccess) printf(\"Error: %s\\n\", cudaGetErrorString(asyncErr));\n",
        "    // swap the temperature pointers\n",
        "    temp_tmp = temp1;\n",
        "    temp1 = temp2;\n",
        "    temp2= temp_tmp;\n",
        "  }\n",
        "  \n",
        "\n",
        "  float maxError = 0;\n",
        "  // Output should always be stored in the temp1 and temp1_ref at this point\n",
        "  for( int i = 0; i < ni*nj; ++i ) {\n",
        "    // printf(\"%.5f \", temp1[i]);\n",
        "    // printf(\"%.5f \\n\", temp1_ref[i]);\n",
        "    if (abs(temp1[i]-temp1_ref[i]) > maxError) { maxError = abs(temp1[i]-temp1_ref[i]); }\n",
        "  }\n",
        "\n",
        "  // Check and see if our maxError is greater than an error bound\n",
        "  if (maxError > 0.0005f){\n",
        "    printf(\"Problem! The Max Error of %.5f is NOT within acceptable bounds.\\n\", maxError);}\n",
        "  else\n",
        "    printf(\"The Max Error of %.5f is within acceptable bounds.\\n\", maxError);\n",
        "\n",
        "\n",
        "  free( temp1_ref );\n",
        "  free( temp2_ref );\n",
        "  cudaFree( temp1 );\n",
        "  cudaFree( temp2 );\n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uf9hNp4p96r9",
        "outputId": "a6b2dfc8-30a6-4610-fa74-5c1a55bc79ab"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting heat_gpu.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_70 -o heat_gpu heat_gpu.cu -run "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLAK_OGt99dv",
        "outputId": "ccc1a178-63fd-4485-a059-9768ba330f12"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Max Error of 0.00001 is within acceptable bounds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Considerations"
      ],
      "metadata": {
        "id": "mwVyMZK6DITm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using CUDA to parallelize applications represents a significant increase in code optimization, and is quite simple to develop once you learn to use the correct primitives, especially when using unified memory features. It performance is clearly better than executing a sequential code, mainly when it is applied in complex mathematical calculations. So, it is an excellent option to accelerating applications."
      ],
      "metadata": {
        "id": "nBWUisQGDKta"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbxAR_nZ3ey8"
      },
      "source": [
        "# References"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHs1Te143ezA"
      },
      "source": [
        "[1] Duato, J., Pena, A. J., Silla, F., Fernandez, J. C., Mayo, R., & Quintana-Orti, E. S. (2011). Enabling CUDA acceleration within virtual machines using rCUDA. 2011 18th International Conference on High Performance Computing. doi:10.1109/hipc.2011.6152718\n",
        "\n",
        "[2] Barnat, J., Brim, L., Ceka, M., & Lamr, T. (2009). CUDA Accelerated LTL Model Checking. 2009 15th International Conference on Parallel and Distributed Systems. doi:10.1109/icpads.2009.50 "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "B5PcGb-Pt0_j",
        "dGlWZ-SlQObx",
        "UXO8ef2HkTxZ",
        "yWL0CwtrhtIN",
        "5SWiBhXEM9VS",
        "lguZ-65K1VO0",
        "c51q1VYY_r0j"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}